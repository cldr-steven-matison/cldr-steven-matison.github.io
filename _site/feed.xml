<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://cldr-steven-matison.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://cldr-steven-matison.github.io//" rel="alternate" type="text/html" /><updated>2025-02-27T09:05:36-05:00</updated><id>https://cldr-steven-matison.github.io//feed.xml</id><title type="html">Cloudera Solutions Engineer</title><subtitle>Solutions Engineer @Cloudera</subtitle><author><name>Steven Matison</name></author><entry><title type="html">Cloudera Apache NiFi Operator</title><link href="https://cldr-steven-matison.github.io//blog/Cloudera-Apache-NiFi-Operator/" rel="alternate" type="text/html" title="Cloudera Apache NiFi Operator" /><published>2025-02-26T00:00:00-05:00</published><updated>2025-02-26T00:00:00-05:00</updated><id>https://cldr-steven-matison.github.io//blog/Cloudera%20Apache%20NiFi%20Operator</id><content type="html" xml:base="https://cldr-steven-matison.github.io//blog/Cloudera-Apache-NiFi-Operator/"><![CDATA[<p>:warning: This is a Work In Progress article, be sure to check back again soon if you see this notation. :warning:</p>

<p>In a previous article (<a href="https://cldr-steven-matison.github.io/blog/Install-CFM-Operator/">Installing Cloudera CFM Kubernetes Operator</a>) I exposed steps necessary to deploy the Cloudera Apache NiFi Operator on MiniKube.  In this article I am going to share some tips and tricks I have learned after completing this install in Openshift, Rancher, and recently EKS.</p>

<figure>
  <img src="/assets/images/cfm-op-deployment-architecture.jpg" />
  <figcaption>CFM Deployment Architecture</figcaption>
</figure>

<p>First, lets start with the most recent version of the documenation for the Cloudera Apache NiFi Operator:</p>

<p><a href="https://docs.cloudera.com/cfm-operator/2.9.1/index.html">CFM Operator 2.9.1</a></p>

<p>ðŸ’¡ There are already two new versions since my first article! ðŸ’¡</p>

<h1 id="user-authentication">User Authentication</h1>

<p>There are several ways that you can role nifi user authentication.  First, no auth at all.  Just access the ui and NiFi is there.  Second, a self generated username and password upon install. Third, provide a kubernetes secret with your desired username and password.  Last but not least, <a href="https://docs.cloudera.com/cfm-operator/2.9.1/configure-nifi-cr/topics/cfm-op-config-nifi-ic-ldap.html">LDAP</a> which I will cover in a future post specifically.</p>

<p><a href="https://docs.cloudera.com/cfm-operator/2.9.1/configure-nifi-cr/topics/cfm-op-configure-nifi-auth.html">Configuring Authentication Docs</a></p>

<p>In order to secure the login for the NiFi UI, it is required that NiFi installation itself be secured.  Be sure to complete this step w/ your security cert issuer and then provide the correct tag in the nifi yaml.   In this example I am using a self signed cert, which you can find in my YAML repo at the end of this page.</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kubectl</span> <span class="n">apply</span> <span class="o">-</span><span class="n">f</span> <span class="nb">self</span><span class="o">-</span><span class="n">signed</span><span class="o">-</span><span class="n">ca</span><span class="o">-</span><span class="n">issuer</span><span class="p">.</span><span class="nf">yaml</span> 
</code></pre></div></div>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="ss">security:
    initialAdminIdentity: </span><span class="n">nifiadmin</span>
    <span class="ss">nodeCertGen:
      issuerRef:
        name: </span><span class="nb">self</span><span class="o">-</span><span class="n">signed</span><span class="o">-</span><span class="n">ca</span><span class="o">-</span><span class="n">issuer</span>
        <span class="ss">kind: </span><span class="no">ClusterIssuer</span>
</code></pre></div></div>

<p>With the cert applied and referenced in the nifi.yaml, lets take a look at the requirements for enabling authentication methods.</p>

<h2 id="auto-generated-password">Auto Generated Password</h2>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ss">spec:
  security:
    singleUserAuth:
      enabled: </span><span class="kp">true</span>
</code></pre></div></div>

<h2 id="provided-credential">Provided Credential</h2>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ss">spec:
  security:
    singleUserAuth:
      enabled: </span><span class="kp">true</span>
      <span class="ss">credentialsSecretName: </span><span class="n">nifi</span><span class="o">-</span><span class="n">credential</span>

</code></pre></div></div>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kubectl</span> <span class="n">create</span> <span class="n">secret</span> <span class="n">generic</span> <span class="n">nifi</span><span class="o">-</span><span class="n">credential</span> <span class="o">--</span><span class="n">from</span><span class="o">-</span><span class="n">literal</span><span class="o">=</span><span class="n">username</span><span class="o">=</span><span class="s2">"username"</span> <span class="o">--</span><span class="n">from</span><span class="o">-</span><span class="n">literal</span><span class="o">=</span><span class="n">password</span><span class="o">=</span><span class="s2">"123456789101112"</span>
</code></pre></div></div>

<p>ðŸ’¡ It is important to know, if your kubernetes secret is less than 12 characters, it will be ignored and nifi will still role with an auto generated username and password.ðŸ’¡</p>

<h1 id="ingress-and-uiconnection">Ingress and uiConnection</h1>

<p>The most important part of the Apache NiFi Operator installation are the steps required to expose the NiFI UI.</p>

<h2 id="route">Route</h2>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="ss">uiConnection:
    type: </span><span class="no">Route</span>
    <span class="ss">routeConfig:
      tls:
        termination: </span><span class="n">passthrough</span>

  <span class="ss">uiConnection:
    type: </span><span class="no">Route</span>
    <span class="ss">serviceConfig:
      sessionAffinity: </span><span class="no">ClientIP</span>
    <span class="ss">routeConfig:
      tls:
        termination: </span><span class="n">passthrough</span>
</code></pre></div></div>

<h2 id="ingress">Ingress</h2>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="ss">uiConnection:
    type: </span><span class="no">Ingress</span>
    <span class="ss">serviceConfig:
      sessionAffinity: </span><span class="no">ClientIP</span>
</code></pre></div></div>

<p>When using an ingress, there are annotations that may or may not be needed depending on your kubernetes environment</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ss">uiConnection:
    type: </span><span class="no">Ingress</span>
    <span class="ss">annotations:
      </span><span class="n">nginx</span><span class="p">.</span><span class="nf">ingress</span><span class="p">.</span><span class="nf">kubernetes</span><span class="p">.</span><span class="nf">io</span><span class="o">/</span><span class="ss">affinity: </span><span class="n">cookie</span>
      <span class="n">nginx</span><span class="p">.</span><span class="nf">ingress</span><span class="p">.</span><span class="nf">kubernetes</span><span class="p">.</span><span class="nf">io</span><span class="o">/</span><span class="n">affinity</span><span class="o">-</span><span class="ss">mode: </span><span class="n">persistent</span>
      <span class="n">nginx</span><span class="p">.</span><span class="nf">ingress</span><span class="p">.</span><span class="nf">kubernetes</span><span class="p">.</span><span class="nf">io</span><span class="o">/</span><span class="n">backend</span><span class="o">-</span><span class="ss">protocol: </span><span class="no">HTTPS</span>
      <span class="n">nginx</span><span class="p">.</span><span class="nf">ingress</span><span class="p">.</span><span class="nf">kubernetes</span><span class="p">.</span><span class="nf">io</span><span class="o">/</span><span class="n">ssl</span><span class="o">-</span><span class="ss">passthrough: </span><span class="s2">"true"</span>
      <span class="n">nginx</span><span class="p">.</span><span class="nf">ingress</span><span class="p">.</span><span class="nf">kubernetes</span><span class="p">.</span><span class="nf">io</span><span class="o">/</span><span class="n">ssl</span><span class="o">-</span><span class="ss">redirect: </span><span class="s2">"true"</span>
</code></pre></div></div>

<h2 id="service">Service</h2>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="ss">uiConnection:
    type: </span><span class="no">Service</span>
</code></pre></div></div>

<h1 id="when-to-delete-and-apply">When to Delete and Apply</h1>

<p>It is important to know when to delete your nifi deployment and apply again for a fresh install.  When doing things like above with authentication and security the initial nifi installation takes different and appropriate paths.   When you change these things, delete nifi, wait for termination to complete, and then apply again in order to take a full fresh install.   Be careful when applying changes and expecting the changed yaml to fully re-install nifi.</p>

<h1 id="customizations-for-nifi-sizing">Customizations For NiFi Sizing</h1>

<p>When testing deployment processes in a small or limited kubernetes environment, it may be required to provide some limits on the resources nifi needs.</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ss">resources:
    nifi:
      requests:
        cpu: </span><span class="s2">"1"</span>
        <span class="ss">memory: </span><span class="mi">2</span><span class="no">Gi</span>
      <span class="ss">limits:
        cpu: </span><span class="s2">"4"</span>
        <span class="ss">memory: </span><span class="mi">4</span><span class="no">Gi</span>
    <span class="ss">log:
      requests:
        cpu: </span><span class="mi">50</span><span class="n">m</span>
        <span class="ss">memory: </span><span class="mi">128</span><span class="no">Mi</span>
</code></pre></div></div>

<p>Some important Sizing docs:</p>

<p><a href="https://docs.cloudera.com/cfm-operator/2.9.1/configure-nifi-cr/topics/cfm-op-configure-nifi-cr-resource.html">Resource Recommendations</a> and <a href="https://docs.cloudera.com/cfm-operator/2.9.1/configure-nifi-cr/topics/cfm-op-configure-nifi-cr-cluster.html">Configuring Cluster Size</a></p>

<p>You can find a full example NiFI chart <a href="https://docs.cloudera.com/cfm-operator/2.9.1/configure-nifi-cr/topics/cfm-op-resource-example.html">here</a>.</p>

<p>If you have gotten this far, then you may want to bookmark the <a href="https://docs.cloudera.com/cfm-operator/2.9.1/nifi-config-reference/topics/cfm-op-nifi-config-reference.html">NiFi Config Reference</a> and <a href="https://docs.cloudera.com/cfm-operator/2.9.1/connection-reference/topics/cfm-op-nifi-connection-reference.html">NiFi Connection Reference</a> which outline all of the yaml object types for the Cloudera Apache NiFi Operator.</p>

<p>Check out this repo I created with my sample YAMLs <a href="https://github.com/cldr-steven-matison/ClouderaOperatorYAML">here</a>.</p>

<p>If you are interested in getting your hands on Clouderaâ€™s Apache NiFi Operator you can find more right <a href="https://www.cloudera.com/products/dataflow.html">here</a>.  You can also reach out to me directly if you are ready for demos, hands on labs, or licensed trials for your organization.</p>]]></content><author><name>Steven Matison</name></author><category term="blog" /><category term="kubernetes" /><category term="operator" /><category term="nifi" /><summary type="html"><![CDATA[:warning: This is a Work In Progress article, be sure to check back again soon if you see this notation. :warning:]]></summary></entry><entry><title type="html">Cloudera DataFlow - Nifi 2.0 Python Processors</title><link href="https://cldr-steven-matison.github.io//blog/NIFI-2-Python-Processor/" rel="alternate" type="text/html" title="Cloudera DataFlow - Nifi 2.0 Python Processors" /><published>2025-02-19T00:00:00-05:00</published><updated>2025-02-19T00:00:00-05:00</updated><id>https://cldr-steven-matison.github.io//blog/NIFI-2-Python-Processor</id><content type="html" xml:base="https://cldr-steven-matison.github.io//blog/NIFI-2-Python-Processor/"><![CDATA[<p>Recently a task came up for one of my customers whom asked how to rebuild python processes they have running with ExecuteScript or ExecuteStreamCommand within NiFi 2.0 as a new python processor.  This customer already has deep experience with nifi, python, and even custom java processors.   As a user of Cloudera Dataflow it has been a very long time since I have had to build and make my own nifi processors.  I also have not had to manage any nifi file systems behind these custom processors.   This is because Cloudera DataFlow allows me to nifi without all the hard parts of the old nifi experience.   Historically making a custom nifi processor was super complicated java and required developer tools, IDEs and deep programming knowledge around the inner workings of nifi source code.  Not anymore with NiFi 2.0â€¦</p>

<p>In this blog I am going to show how easy it is to feed python source code on s3 into a nifi data flow without all those sharpe edges.</p>

<p>First,  I need to research the upstream <a href="https://nifi.apache.org/nifi-docs/python-developer-guide.html">documentation</a> for examples.  This one is a great starting point CreateFlowfile:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">from</span> <span class="n">nifiapi</span><span class="p">.</span><span class="nf">flowfilesource</span> <span class="n">import</span> <span class="no">FlowFileSource</span><span class="p">,</span> <span class="no">FlowFileSourceResult</span>

<span class="k">class</span> <span class="nc">CreateFlowFile</span><span class="p">(</span><span class="no">FlowFileSource</span><span class="p">):</span>
    <span class="k">class</span> <span class="nc">Java</span><span class="p">:</span>
        <span class="n">implements</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'org.apache.nifi.python.processor.FlowFileSource'</span><span class="p">]</span>

    <span class="k">class</span> <span class="nc">ProcessorDetails</span><span class="p">:</span>
        <span class="n">version</span> <span class="o">=</span> <span class="s1">'0.0.1-SNAPSHOT'</span>
        <span class="n">description</span> <span class="o">=</span> <span class="s1">'''A Python processor that creates FlowFiles.'''</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">pass</span>

    <span class="k">def</span> <span class="nf">create</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="k">return</span> <span class="no">FlowFileSourceResult</span><span class="p">(</span><span class="n">relationship</span> <span class="o">=</span> <span class="s1">'success'</span><span class="p">,</span> <span class="n">attributes</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'greeting'</span><span class="p">:</span> <span class="s1">'hello'</span><span class="p">},</span> <span class="n">contents</span> <span class="o">=</span> <span class="s1">'Hello World!'</span><span class="p">)</span>
</code></pre></div></div>
<p>Next I need to build out the basic samples that validate the documented process works to deploy a custom python processor.  I delivered the above code to s3 and use that in my deployment:</p>

<figure>
  <img src="/assets/images/nifi-python-processor-storage-location-1.png" />
  <figcaption>Cloudera DataFlow Custom Python Processsor Storage Location</figcaption>
</figure>

<p>I am further automating this configuration so the deployment looks like this:</p>
<figure>
  <img src="/assets/images/nifi-python-processor-storage-location-2.png" />
  <figcaption>Cloudera DataFlow Custom Python Processsor Storage Location</figcaption>
</figure>

<p>Now when I deploy this flow, this processor is available:</p>

<figure>
  <img src="/assets/images/nifi-python-processor-createflowfile.png" />
  <figcaption>Cloudera DataFlow Custom Python Processsor Storage Location</figcaption>
</figure>

<p>Next, I can focus on modifying the working example for my Fraud Detection use case.   That sample script is well tested and its python source code has been used within an ExecuteScript Processor.  I just need to make some very basic changes to merge my example python processor with the same python from my ExecuteScript processor.</p>

<p>My final python processor</p>

<figure>
  <img src="/assets/images/nifi-python-processor-transaction-generator.png" />
  <figcaption>Cloudera DataFlow Custom Python Processsor Storage Location</figcaption>
</figure>

<p>And the source code is closely as follows:</p>
<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># TransactionGenerator.py</span>
<span class="n">from</span> <span class="n">nifiapi</span><span class="p">.</span><span class="nf">flowfilesource</span> <span class="n">import</span> <span class="no">FlowFileSource</span><span class="p">,</span> <span class="no">FlowFileSourceResult</span>
<span class="n">import</span> <span class="n">sys</span>
<span class="n">import</span> <span class="n">os</span>
<span class="n">import</span> <span class="n">socket</span>
<span class="n">import</span> <span class="n">logging</span>
<span class="n">import</span> <span class="n">string</span>
<span class="n">import</span> <span class="n">datetime</span>
<span class="n">import</span> <span class="n">random</span>
<span class="n">import</span> <span class="n">uuid</span>
<span class="n">import</span> <span class="n">csv</span>
<span class="n">import</span> <span class="n">json</span>
<span class="n">import</span> <span class="n">math</span>
<span class="n">import</span> <span class="n">time</span>
<span class="n">from</span> <span class="n">random</span> <span class="n">import</span> <span class="n">randint</span>
<span class="n">from</span> <span class="n">random</span> <span class="n">import</span> <span class="n">uniform</span>

<span class="c1"># Add some data = Amounts and Cities.</span>
<span class="no">AMOUNTS</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span><span class="mi">500</span><span class="p">]</span>
<span class="no">CITIES</span> <span class="o">=</span> <span class="p">[</span>                                                                                                                                                                                                                                                     
    <span class="p">{</span><span class="s2">"lat"</span><span class="p">:</span> <span class="mf">48.8534</span><span class="p">,</span> <span class="s2">"lon"</span><span class="p">:</span> <span class="mf">2.3488</span><span class="p">,</span> <span class="s2">"city"</span><span class="p">:</span> <span class="s2">"Paris"</span><span class="p">},</span>                                                                                                                                                                                                    
    <span class="p">{</span><span class="s2">"lat"</span><span class="p">:</span> <span class="mf">43.2961743</span><span class="p">,</span> <span class="s2">"lon"</span><span class="p">:</span> <span class="mf">5.3699525</span><span class="p">,</span> <span class="s2">"city"</span><span class="p">:</span> <span class="s2">"Marseille"</span><span class="p">},</span>                                                                                                                                                                                                 
    <span class="p">{</span><span class="s2">"lat"</span><span class="p">:</span> <span class="mf">45.7578137</span><span class="p">,</span> <span class="s2">"lon"</span><span class="p">:</span> <span class="mf">4.8320114</span><span class="p">,</span> <span class="s2">"city"</span><span class="p">:</span> <span class="s2">"Lyon"</span><span class="p">},</span>                                                                                                                                                                                                      
    <span class="p">{</span><span class="s2">"lat"</span><span class="p">:</span> <span class="mf">50.6365654</span><span class="p">,</span> <span class="s2">"lon"</span><span class="p">:</span> <span class="mf">3.0635282</span><span class="p">,</span> <span class="s2">"city"</span><span class="p">:</span> <span class="s2">"Lille"</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">"lat"</span><span class="p">:</span> <span class="mf">44.841225</span><span class="p">,</span> <span class="s2">"lon"</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.5800364</span><span class="p">,</span> <span class="s2">"city"</span><span class="p">:</span> <span class="s2">"Bordeaux"</span><span class="p">}</span>
<span class="p">]</span>   

<span class="k">class</span> <span class="nc">TransactionGenerator</span><span class="p">(</span><span class="no">FlowFileSource</span><span class="p">):</span>
    <span class="k">class</span> <span class="nc">Java</span><span class="p">:</span>
        <span class="n">implements</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'org.apache.nifi.python.processor.FlowFileSource'</span><span class="p">]</span>

    <span class="k">class</span> <span class="nc">ProcessorDetails</span><span class="p">:</span>
        <span class="n">version</span> <span class="o">=</span> <span class="s1">'0.0.1-SNAPSHOT'</span>
        <span class="n">description</span> <span class="o">=</span> <span class="s1">'''A Python processor that creates credit card transactions for the Fraud Demo.'''</span>

    <span class="c1"># Define geo functions</span>
    <span class="k">def</span> <span class="nf">create_random_point</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">distance</span><span class="p">):</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">distance</span><span class="o">/</span><span class="mi">111300</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">pi</span> <span class="o">*</span> <span class="n">v</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">y0</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x0</span><span class="o">+</span><span class="n">x1</span><span class="p">,</span> <span class="n">y0</span> <span class="o">+</span><span class="n">y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">create_geopoint</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">lat</span><span class="p">,</span> <span class="n">lon</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">self</span><span class="p">.</span><span class="nf">create_random_point</span><span class="p">(</span><span class="n">lat</span><span class="p">,</span> <span class="n">lon</span><span class="p">,</span> <span class="mi">50000</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_latlon</span><span class="p">(</span><span class="nb">self</span><span class="p">):</span>                                                                    
        <span class="n">geo</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="no">CITIES</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">self</span><span class="p">.</span><span class="nf">create_geopoint</span><span class="p">(</span><span class="n">geo</span><span class="p">[</span><span class="s1">'lat'</span><span class="p">],</span> <span class="n">geo</span><span class="p">[</span><span class="s1">'lon'</span><span class="p">]),</span><span class="n">geo</span><span class="p">[</span><span class="s1">'city'</span><span class="p">]</span>        

    <span class="k">def</span> <span class="nf">create_fintran</span><span class="p">(</span><span class="nb">self</span><span class="p">):</span>
     
        <span class="n">latlon</span><span class="p">,</span><span class="n">city</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">get_latlon</span><span class="p">()</span>
        <span class="n">tsbis</span><span class="o">=</span><span class="p">(</span><span class="n">datetime</span><span class="p">.</span><span class="nf">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">()).</span><span class="nf">strftime</span><span class="p">(</span><span class="s2">"%Y-%m-%d %H:%M:%S "</span><span class="p">)</span>
        <span class="n">date</span> <span class="o">=</span> <span class="n">str</span><span class="p">(</span><span class="n">datetime</span><span class="p">.</span><span class="nf">datetime</span><span class="p">.</span><span class="nf">strptime</span><span class="p">(</span><span class="n">tsbis</span><span class="p">,</span> <span class="s2">"%Y-%m-%d %H:%M:%S "</span><span class="p">))</span>
        <span class="n">fintran</span> <span class="o">=</span> <span class="p">{</span>
          <span class="s1">'ts'</span><span class="p">:</span> <span class="n">date</span><span class="p">,</span>
          <span class="s1">'account_id'</span> <span class="p">:</span> <span class="n">str</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)),</span>
          <span class="s1">'transaction_id'</span> <span class="p">:</span> <span class="n">str</span><span class="p">(</span><span class="n">uuid</span><span class="p">.</span><span class="nf">uuid1</span><span class="p">()),</span>
          <span class="s1">'amount'</span> <span class="p">:</span> <span class="n">random</span><span class="p">.</span><span class="nf">randrange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2000</span><span class="p">),</span>  
          <span class="s1">'lat'</span> <span class="p">:</span> <span class="n">latlon</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
          <span class="s1">'lon'</span> <span class="p">:</span> <span class="n">latlon</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">}</span>    
        <span class="k">return</span> <span class="p">(</span><span class="n">fintran</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">create_fraudtran</span><span class="p">(</span><span class="n">fintran</span><span class="p">):</span>
        <span class="n">latlon</span><span class="p">,</span><span class="n">city</span> <span class="o">=</span> <span class="n">get_latlon</span><span class="p">()</span>
        <span class="n">tsbis</span> <span class="o">=</span> <span class="n">str</span><span class="p">((</span><span class="n">datetime</span><span class="p">.</span><span class="nf">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">datetime</span><span class="p">.</span><span class="nf">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span><span class="mi">600</span><span class="p">))).</span><span class="nf">strftime</span><span class="p">(</span><span class="s2">"%Y-%m-%d %H:%M:%S "</span><span class="p">))</span>
        <span class="n">fraudtran</span> <span class="o">=</span> <span class="p">{</span>
          <span class="s1">'ts'</span> <span class="p">:</span> <span class="n">tsbis</span><span class="p">,</span>
          <span class="s1">'account_id'</span> <span class="p">:</span> <span class="n">fintran</span><span class="p">[</span><span class="s1">'account_id'</span><span class="p">],</span>
          <span class="s1">'transaction_id'</span> <span class="p">:</span> <span class="s1">'xxx'</span> <span class="o">+</span> <span class="n">str</span><span class="p">(</span><span class="n">fintran</span><span class="p">[</span><span class="s1">'transaction_id'</span><span class="p">]),</span>
          <span class="s1">'amount'</span> <span class="p">:</span> <span class="n">random</span><span class="p">.</span><span class="nf">randrange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2000</span><span class="p">),</span>      
          <span class="s1">'lat'</span> <span class="p">:</span> <span class="n">latlon</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
          <span class="s1">'lon'</span> <span class="p">:</span> <span class="n">latlon</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">}</span>    
        <span class="k">return</span> <span class="p">(</span><span class="n">fraudtran</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">pass</span>

    <span class="k">def</span> <span class="nf">create</span><span class="p">(</span><span class="nb">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="n">fintran</span> <span class="o">=</span> <span class="nb">self</span><span class="p">.</span><span class="nf">create_fintran</span><span class="p">()</span>   
        <span class="n">fintransaction</span> <span class="o">=</span>  <span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">fintran</span><span class="p">)</span>
        <span class="k">return</span> <span class="no">FlowFileSourceResult</span><span class="p">(</span><span class="n">relationship</span> <span class="o">=</span> <span class="s1">'success'</span><span class="p">,</span> <span class="n">attributes</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'NiFi'</span><span class="p">:</span> <span class="s1">'PythonProcessor'</span><span class="p">},</span> <span class="n">contents</span> <span class="o">=</span> <span class="n">fintransaction</span><span class="p">)</span>
</code></pre></div></div>

<p>Not bad, I now have an example to share with my customer, and I can modify my demos and hands on labs to use this processor instead of ExecuteScript.</p>

<p>Check out the Cloudera DataFlow <a href="https://docs.cloudera.com/dataflow/cloud/custom-processors/topics/cdf-bp-custom-python-processors.html">DOCS</a> for custom python processors.</p>]]></content><author><name>Steven Matison</name></author><category term="blog" /><category term="cloudera" /><category term="dataflow" /><category term="nifi" /><summary type="html"><![CDATA[Recently a task came up for one of my customers whom asked how to rebuild python processes they have running with ExecuteScript or ExecuteStreamCommand within NiFi 2.0 as a new python processor. This customer already has deep experience with nifi, python, and even custom java processors. As a user of Cloudera Dataflow it has been a very long time since I have had to build and make my own nifi processors. I also have not had to manage any nifi file systems behind these custom processors. This is because Cloudera DataFlow allows me to nifi without all the hard parts of the old nifi experience. Historically making a custom nifi processor was super complicated java and required developer tools, IDEs and deep programming knowledge around the inner workings of nifi source code. Not anymore with NiFi 2.0â€¦]]></summary></entry><entry><title type="html">Cloudera Flow Management 4.0.0 Technical Preview for Cloudera 7.3.1</title><link href="https://cldr-steven-matison.github.io//blog/Cloudera-Flow-Management-4.0.0/" rel="alternate" type="text/html" title="Cloudera Flow Management 4.0.0 Technical Preview for Cloudera 7.3.1" /><published>2025-01-21T00:00:00-05:00</published><updated>2025-01-21T00:00:00-05:00</updated><id>https://cldr-steven-matison.github.io//blog/Cloudera-Flow-Management-4.0.0</id><content type="html" xml:base="https://cldr-steven-matison.github.io//blog/Cloudera-Flow-Management-4.0.0/"><![CDATA[<p>The Data in Motion (DiM) team is pleased to announce the release of Cloudera Flow Management 4.0.0 as a Technical Preview for running Apache NiFi 2.0 with Cloudera Manager on Cloudera 7.1.9 and 7.3.1 Private Cloud Base clusters (all service packs).</p>

<p>This release offers a number of new features and improves stability and security through bug fixes and upgraded dependencies.</p>

<h1 id="key-features-for-this-release">Key features for this release</h1>
<ul>
  <li>NiFi 2.0: Apache NiFi 2.0 comes with a number of structural and security improvements that allow for more robust deployments as well as upgraded libraries providing several security enhancements and addressing many common vulnerabilities.</li>
  <li>Native Python processor development: NiFi 2.0 provides a Python SDK for which processors can be rapidly developed in Python and deployed in flows. Some common document parsing processors written in Python are included in the release.</li>
  <li>Flow Analysis Rules Engine: NiFi 2.0 provides a rules engine for developing flow analysis rules that recommend and enforce Best Practices for flow design. CFM 4.0 provides several Flow Analysis Rules for such aspects as thread management and recommended components.</li>
  <li>Critical Component Support: CFM 4 includes several components that were removed in Apache NiFi 2.0 such as Hive, Iceberg, Ranger, and Kudu. CFM 4.0 continues to support all aspects of the Cloudera Data Platform (CDP), and going forward, Clouderaâ€™s distribution of NiFi 2.0 will be the only possibility to get access to and support for these components.</li>
  <li>Updates and Patches: And, as usual, bug fixes, security patches, performance improvements, etc.</li>
</ul>

<h1 id="support-matrix-and-upgrade-paths">Support Matrix and Upgrade Paths</h1>
<p>Users will be able in the future to exercise the upcoming migration tool to convert existing flows to run on CFM 4 instances. CFM 4 is supported on CDP 7.1.9 and CDP 7.3.1.</p>

<h1 id="use-cases">Use Cases</h1>

<p>Leverage NiFi 2.0 capabilities with Cloudera enhancements:</p>
<ul>
  <li>Try out new NiFi 2 capabilities with Clouderaâ€™s first NiFi 2 based on-prem release (in Technical Preview)</li>
  <li>Flow administrators can enforce Best Practices for flow designers to ensure robust, reliable solutions</li>
  <li>Create RAG pipelines for GenAI, Data Engineering, and Machine Learning model scoring using new processors and integrations</li>
</ul>

<p>Check out the <a href="https://docs.cloudera.com/cfm/4.0.0/release-notes/topics/cfm-whats-new.html">Release Notes</a></p>

<p>As always, check out the entire <a href="https://docs.cloudera.com/cfm/4.0.0/index.html">DOCS</a> for the Cloudera Flow Management.</p>]]></content><author><name>Steven Matison</name></author><category term="blog" /><category term="cloudera" /><category term="nifi" /><summary type="html"><![CDATA[The Data in Motion (DiM) team is pleased to announce the release of Cloudera Flow Management 4.0.0 as a Technical Preview for running Apache NiFi 2.0 with Cloudera Manager on Cloudera 7.1.9 and 7.3.1 Private Cloud Base clusters (all service packs).]]></summary></entry><entry><title type="html">Cloudera Migration Assistant 3.5.0</title><link href="https://cldr-steven-matison.github.io//blog/Cloudera-Migration-Assistant-3.5.0/" rel="alternate" type="text/html" title="Cloudera Migration Assistant 3.5.0" /><published>2025-01-16T00:00:00-05:00</published><updated>2025-01-16T00:00:00-05:00</updated><id>https://cldr-steven-matison.github.io//blog/Cloudera-Migration-Assistant%203.5.0</id><content type="html" xml:base="https://cldr-steven-matison.github.io//blog/Cloudera-Migration-Assistant-3.5.0/"><![CDATA[<p>Cloudera is excited to announce the General Availability of Cloudera Migration Assistant with the release of the 3.5.0 version. Clouderaâ€™s go-to migration tool now supports migrating Spark2 and Spark3 workloads from Cloudera on premises to Cloudera on cloud, as well as automated execution of migrated workloads with Livy, for a seamless workload migration audit experience.</p>

<p>Clouderaâ€™s vision is to make Cloudera Migration Assistant the de facto standard tool for cloud migration to Cloudera on cloud, enabling a streamlined, assisted, automated experience.</p>

<h1 id="key-features-and-capabilities">Key Features and Capabilities:</h1>

<ul>
  <li>Redesigned Migration View: The main user interface of Cloudera Migration Assistant got a significant update for easier navigation throughout the migration process.</li>
  <li>Automatic Discovery: Cloudera Migration Assistant detects HDFS files as well as HMS and HBase tables in source clusters for a seamless migration setup.</li>
  <li>Workload Identification: Supports migration of SQL, Oozie, and Spark workloads, ensuring comprehensive coverage.</li>
  <li>Dependency Mapping and Planning: Assists in identifying dependencies and planning migration strategies with precision.</li>
  <li>Controlled Execution: Enables managed and observable execution of complex migrations for greater transparency.</li>
  <li>Reduced Manual Effort: Streamlined automation reduces the need for manual intervention, increasing efficiency.</li>
</ul>

<p>Version 3.5.0 introduces new functionalities, implements a more modern architecture, increases product security posture, and adopts a streamlined development process to deliver updates to market more quickly. For organizations looking to migrate from legacy Cloudera Distribution of Hadoop (CDH) or Hortonworks Data Platform (HDP) deployments, version 3.4.1 remains the Cloudera Migration Assistant of choice.</p>

<p>To further explore Cloudera Migration Assistant, check out the resources below.</p>

<p>Check out the <a href="https://docs.cloudera.com/cdp-public-cloud/cloud/cma-cdh-cdppc-migration/topics/cma-tool-overview.html#ariaid-title3">Release Notes</a></p>

<p>As always, check out the entire <a href="https://docs.cloudera.com/cdp-public-cloud/cloud/cma-cdh-cdppc-migration/topics/cma-tool-overview.html#unique_448263855">DOCS</a> for the Cloudera Migration Assistant.</p>]]></content><author><name>Steven Matison</name></author><category term="blog" /><category term="cloudera" /><category term="spark" /><category term="kubernetes" /><category term="cdp" /><summary type="html"><![CDATA[Cloudera is excited to announce the General Availability of Cloudera Migration Assistant with the release of the 3.5.0 version. Clouderaâ€™s go-to migration tool now supports migrating Spark2 and Spark3 workloads from Cloudera on premises to Cloudera on cloud, as well as automated execution of migrated workloads with Livy, for a seamless workload migration audit experience.]]></summary></entry><entry><title type="html">Cloudera Streams Messaging - Kubernetes Operator 1.2</title><link href="https://cldr-steven-matison.github.io//blog/CSM-Kubernetes-Operator1-2-Release/" rel="alternate" type="text/html" title="Cloudera Streams Messaging - Kubernetes Operator 1.2" /><published>2024-12-23T00:00:00-05:00</published><updated>2024-12-23T00:00:00-05:00</updated><id>https://cldr-steven-matison.github.io//blog/CSM-Kubernetes-Operator1-2-Release</id><content type="html" xml:base="https://cldr-steven-matison.github.io//blog/CSM-Kubernetes-Operator1-2-Release/"><![CDATA[<p>Clouderaâ€™s Data In Motion Team is pleased to announce the release of Cloudera Streams Messaging - Kubernetes Operator 1.2, an integral component of Cloudera Streaming - Kubernetes Operator. With this release, customers receive better security integration and an update to Kafka 3.8, besides other improvements.</p>

<h2 id="release-highlights">Release Highlights</h2>

<ul>
  <li>Rebase on Kafka 3.8:
    <ul>
      <li>For more information, see the Kafka 3.8 <a href="https://archive.apache.org/dist/kafka/3.8.0/RELEASE_NOTES.html">Release Notes</a> and <a href="https://kafka.apache.org/documentation/#upgrade_380_notable">list of notable changes</a>.</li>
    </ul>
  </li>
  <li>Rebase on Strimzi 0.43.0:
    <ul>
      <li>For more information, see the Strimzi 0.43.0 <a href="https://github.com/strimzi/strimzi-kafka-operator/releases/tag/0.43.0">Release Notes</a>.</li>
    </ul>
  </li>
  <li>Apache Ranger authorization: Support for Apache Ranger authorization is now available. Customers can now integrate Kafka clusters, deployed with Cloudera Streams Messaging - Kubernetes Operator, with a remote Ranger service that is running on Cloudera Private Cloud Base. If configured, the Ranger service can provide authorization for your Kafka cluster.
    <ul>
      <li>For more information, see <a href="https://docs.cloudera.com/csm-operator/1.2/kafka-security/topics/csm-op-authz-ranger.html">Apache Ranger authorization</a>.</li>
    </ul>
  </li>
  <li>
    <p>Improvements to Kafka replication: Rebased and backported changes to make Kafka replication more resilient and reliable when handling heartbeats and offset translation.</p>
  </li>
  <li>Performance improvements for the Cloudera diagnostics tool: The report.sh tool, used by clients to provide Cloudera support with key information when dealing with support cases now runs its subprocesses in parallel, accelerating run times.
    <ul>
      <li>For more information, see <a href="https://docs.cloudera.com/csm-operator/1.2/monitoring-diagnostics/topics/csm-op-diagnostics.html">Diagnostics</a>.</li>
    </ul>
  </li>
</ul>

<p>For the complete list of fixes and improvements read these <a href="https://docs.cloudera.com/csm-operator/1.2/release-notes/topics/csm-op-rn.html">Release Notes</a>.</p>

<p>Getting to the new release
To upgrade to Cloudera Streams Messaging - Kubernetes Operator 1.2, check out this <a href="https://docs.cloudera.com/csm-operator/1.2/upgrade/topics/csm-op-upgrade.html">upgrade guide</a>. Please note, if you are installing for the first time use this installation overview.</p>

<h2 id="use-cases">Use Cases</h2>
<ul>
  <li>
    <p>Flexible, agile, and rapid Kafka deployments: Deploy Apache Kafka in seconds on existing Kubernetes infrastructure. Cloudera Streams Messaging - Kubernetes Operator has very lightweight dependencies and system requirements for Kafka-centric deployments. It simplifies and standardizes Kafka deployments and provides auto-scaling support for variable workloads.</p>
  </li>
  <li>
    <p>Operational efficiency with simple upgrades: The complexity of Kafka rolling upgrades is handled by Cloudera Streams Messaging - Kubernetes Operator, making them simpler and safer to execute.</p>
  </li>
  <li>
    <p>Loading and unloading data from Kafka: Kafka Connect gives Kafka users a simple way to access data quickly from a source and feed it to a Kafka topic. It also allows them to get data from a topic and copy it to an external destination. The operator includes Kafka Connect support to give our customers a tool for moving data in and out of Kafka, efficiently.</p>
  </li>
  <li>
    <p>Replicating data to other sites: Disaster resilience is an important aspect of any Kafka production deployment. Cloudera Streams Messaging - Kubernetes Operator supports configuring and running Kafka replication flows across any two Kafka clusters. These clusters could be in the same or in different data centers to provide increased resilience against disasters.</p>
  </li>
  <li>
    <p>Kafka migrations: Customers can migrate or replicate data between containerized Kafka clusters and on-prem or cloud-based clusters.  Using Cloudera Streams Messaging - Kubernetes Operator, data can be replicated in any direction and between two or more clusters at a time.</p>
  </li>
</ul>

<p>Check out what is new in <a href="https://docs.cloudera.com/csm-operator/1.2/release-notes/topics/csm-op-rn.html#ariaid-title2">Cloudera Streams Messaging Kubernetes Operator 1.2</a></p>

<p>Check out <a href="https://docs.cloudera.com/csm-operator/1.2/overview/topics/csm-op-deployment-architecture.html">Deployment Architecture</a></p>

<p>Check out <a href="https://docs.cloudera.com/csm-operator/1.2/installation/topics/csm-op-install-overview.html">Installation Information</a></p>

<p>As always, check out the entire <a href="https://docs.cloudera.com/csm-operator/1.2/index.html">DOCS</a> for the CSM Operator.</p>]]></content><author><name>Steven Matison</name></author><category term="blog" /><category term="csm" /><category term="kafka" /><category term="kubernetes" /><category term="cdp" /><summary type="html"><![CDATA[Clouderaâ€™s Data In Motion Team is pleased to announce the release of Cloudera Streams Messaging - Kubernetes Operator 1.2, an integral component of Cloudera Streaming - Kubernetes Operator. With this release, customers receive better security integration and an update to Kafka 3.8, besides other improvements.]]></summary></entry><entry><title type="html">Cloudera Streaming Analytics 1.14 for Cloudera Data Platform 7.3.1</title><link href="https://cldr-steven-matison.github.io//blog/CSA-1.14-Release/" rel="alternate" type="text/html" title="Cloudera Streaming Analytics 1.14 for Cloudera Data Platform 7.3.1" /><published>2024-12-16T00:00:00-05:00</published><updated>2024-12-16T00:00:00-05:00</updated><id>https://cldr-steven-matison.github.io//blog/CSA-1.14-Release</id><content type="html" xml:base="https://cldr-steven-matison.github.io//blog/CSA-1.14-Release/"><![CDATA[<p>The Data In Motion team is pleased to announce the release of the Cloudera Streaming Analytics 1.14 for Cloudera Public Cloud and Private Cloud Base 7.3.1. This release includes improvements to SQL Stream Builder as well as updates to Apache Flink 1.19.1. These changes are focused on enhancing the user experience and removing objections and blockers in the sales cycle.</p>

<p>See <a href="https://docs.cloudera.com/csa/latest/release-notes/topics/csa-what-new.html">Whatâ€™s New</a>.</p>

<h2 id="release-highlights">Release Highlights</h2>
<ul>
  <li>Rebase to Apache Flink 1.19.1: Streaming analytics deployments, including SQL Stream Builder, now support Apache Flink 1.19.1, which includes the Apache Flink improvements below. For more information on these improvements and deprecations, please check the Apache Flink 1.19.1 release announcement.
    <ul>
      <li>Custom Parallelism for Table/SQL Sources: The DataGen connector now supports setting of custom parallelism for performance tuning via the scan.parallelism option. Support for other connectors will come in future releases.</li>
      <li>Configure Different State Time to Live (TTLs) Using SQL Hint: Users have now a more flexible way to specify custom time-to-live (TTL) values for state of regular joins and group aggregations directly within their queries by utilizing the STATE_TTL hint.</li>
      <li>Named Parameters: Named parameters can now be used when calling a function or stored procedure in Flink SQL.</li>
      <li>Support for SESSION Window Table-Valued Functions (TVFs) in Streaming Mode: Users can now use SESSION Window table-valued functions (TVF) in streaming mode.</li>
      <li>Support for Changelog Inputs for Window TVF Aggregation: Window aggregation operators can now handle changelog streams (e.g., Change Data Capture [CDC] data sources, etc.).</li>
      <li>New UDF Type: AsyncScalarFunction: The new AsyncScalarFunction is a user-defined asynchronous ScalarFunction that allows for issuing concurrent function calls asynchronously.</li>
      <li>MiniBatch Optimization for Regular Joins: The new mini-batch optimization can be used in regular joins to reduce intermediate results, especially in cascading join scenarios.</li>
      <li>Dynamic Source Parallelism Inference for Batch Jobs: Allows source connectors to dynamically infer the parallelism based on the actual amount of data to consume.</li>
      <li>Standard Yet Another Markup Language (YAML) for Apache Flink Configuration: Apache Flink has officially introduced full support for the standard YAML 1.2 syntax in the configuration file.</li>
      <li>Profiling JobManager/TaskManager on Apache Flink Web: Support for triggering profiling at the JobManager/TaskManager level.</li>
      <li>New Config Options for Administrator Java Virtual Machine (JVM) Options: A set of administrator JVM options are available to prepend the user-set JVM options with default values for platform-wide JVM tuning.</li>
      <li>Using Larger Checkpointing Interval When Source is Processing Backlog: Users can set the execution.checkpointing.interval-during-backlog to use a larger checkpoint interval to enhance the throughput while the job is processing backlog if the source is backlog-aware.</li>
      <li>CheckpointsCleaner Clean Individual Checkpoint States in Parallel: Now, when disposing of no longer needed checkpoints, every state handle/state file will be disposed in parallel for better performance.</li>
      <li>Trigger Checkpoints through Command Line Client: The command line interface supports triggering a checkpoint manually.</li>
      <li>New Interfaces to SinkV2 That Are Consistent with Source API.</li>
      <li>New Committer Metrics to Track the Status of Committables.</li>
    </ul>
  </li>
  <li>Support for Python User-Defined Functions (UDFs) in SQL Stream Builder: The current Javascript UDFs in SQL Stream Builder will not work in Java 17 and later versions due to the deprecation and removal of the Nashorn engine from the Java Development Kit (JDK). The addition of Python UDFs to SQL Stream Builder will allow customers to use Python to create new UDFs that will continue to be supported on future JDKs. Javascript UDFs are being deprecated in this release and will be removed in a future release. Cloudera recommends that customers start using Python UDFs for all new development and start migrating their JavaScript UDFs to Python UDFs to prepare for future upgrades.</li>
</ul>

<p>Note: Currently, Cloudera Streaming Analytics 1.14 only supports JDK versions 8 and 11.</p>

<ul>
  <li>
    <p>SQL Stream Builder support for load balancing via Knox for HA deployments: Knox now automatically discovers and provides a load balanced endpoint for SQL Stream Builder when multiple instances of the streaming engine are deployed.</p>
  </li>
  <li>
    <p>Global logging configuration for Configuring logs for all SSB jobs: A new global settings view enables default logging configurations to be set by the administrator. These settings will be applied to all streaming jobs by default and can be overridden at the job level. This ensures that a consistent logging standard can be applied by default for all users and developers.</p>
  </li>
</ul>

<p>Please see the <a href="https://docs.cloudera.com/csa/1.14.0/release-notes/topics/csa-what-new.html">Release Notes</a> for the complete list of fixes and improvements.</p>

<h1 id="use-cases">Use Cases</h1>
<ul>
  <li>
    <p>Event-Driven Applications: Stateful applications that ingest events from one or more event streams and react to incoming events by triggering computations, state updates, or external actions.</p>

    <p>Apache Flink excels in handling the concept of time and state for these applications and can scale to manage very large data volumes (up to several terabytes) with exactly once consistency guarantees. Moreover, Apache Flinkâ€™s support for event-time, highly customizable window logic, and fine-grained control of time as provided by the ProcessFunction enable the implementation of advanced business logic. Moreover, Apache Flink features a library for Complex Event Processing (CEP) to detect patterns in data streams.</p>

    <p>However, Apache Flinkâ€™s outstanding feature for event-driven applications is its support for savepoints. A savepoint is a consistent state image that can be used as a starting point for compatible applications. Given a savepoint, an application can be updated or adapt its scale, or multiple versions of an application can be started for A/B testing.</p>

    <p>Examples:</p>
    <ul>
      <li>Fraud detection</li>
      <li>Anomaly detection</li>
      <li>Rule-based alerting</li>
      <li>Business process monitoring</li>
      <li>Web application (social network)</li>
    </ul>
  </li>
  <li>
    <p>Data Analytics Applications: With a sophisticated stream processing engine, analytics can also be performed in real-time. Streaming queries or applications ingest real-time event streams and continuously produce and update results as events are consumed. The results are written to an external database or maintained as internal state. A dashboard application can read the latest results from the external database or directly query the internal state of the application.</p>

    <p>Apache Flink supports streaming as well as batch analytical applications.</p>

    <p>Examples:</p>
    <ul>
      <li>Quality monitoring of telco networks</li>
      <li>Analysis of product updates &amp; experiment evaluation in mobile applications</li>
      <li>Ad-hoc analysis of live data in consumer technology</li>
      <li>Large-scale graph analysis</li>
    </ul>
  </li>
  <li>
    <p>Data Pipeline Applications: Streaming data pipelines serve a similar purpose as Extract-transform-load (ETL) jobs. They transform and enrich data and can move it from one storage system to another. However, they operate in a continuous streaming mode instead of being periodically triggered. Hence, they can read records from sources that continuously produce data and move it with low latency to their destination.</p>

    <p>Examples:</p>
    <ul>
      <li>Real-time search index building in e-commerce</li>
      <li>Continuous ETL in e-commerce</li>
    </ul>
  </li>
</ul>

<h1 id="getting-to-the-new-release">Getting to the new release</h1>
<p>To upgrade to Cloudera Streaming Analytics 1.14, first ensure that your Cloudera Private Cloud Base environment is already upgraded to version 7.3.1 SP1 and then follow the instructions in the Cloudera Streaming Analytics <a href="https://docs.cloudera.com/csa/1.14.0/installation/topics/csa-upgrade.html">upgrade guide</a>.</p>

<p>Check out <a href="https://docs.cloudera.com/csa/1.14.0/index.html">CSA 1.14</a></p>

<p>Check out <a href="https://docs.cloudera.com/csa/1.14.0/release-notes/topics/csa-what-new.html">Release Notes</a></p>

<p>Check out <a href="https://docs.cloudera.com/csa/1.14.0/download/topics/csa-download-location.html">Download Information</a></p>

<p>As always, check out the <a href="https://docs.cloudera.com/csa/1.14.0/index.html">CSA 1.14 Docs</a></p>]]></content><author><name>Steven Matison</name></author><category term="blog" /><category term="csa" /><category term="flink" /><category term="cdp" /><summary type="html"><![CDATA[The Data In Motion team is pleased to announce the release of the Cloudera Streaming Analytics 1.14 for Cloudera Public Cloud and Private Cloud Base 7.3.1. This release includes improvements to SQL Stream Builder as well as updates to Apache Flink 1.19.1. These changes are focused on enhancing the user experience and removing objections and blockers in the sales cycle.]]></summary></entry><entry><title type="html">Introducing Clouderaâ€™s Unified Runtime with 7.3.1</title><link href="https://cldr-steven-matison.github.io//blog/Cloudera-Unified-Runtime-7.3.1/" rel="alternate" type="text/html" title="Introducing Clouderaâ€™s Unified Runtime with 7.3.1" /><published>2024-12-12T00:00:00-05:00</published><updated>2024-12-12T00:00:00-05:00</updated><id>https://cldr-steven-matison.github.io//blog/Cloudera%20Unified%20Runtime%207.3.1</id><content type="html" xml:base="https://cldr-steven-matison.github.io//blog/Cloudera-Unified-Runtime-7.3.1/"><![CDATA[<p>We are thrilled to announce Clouderaâ€™s first-ever unified release: Cloudera 7.3.1!  This is a single software release available for both cloud and on-premises environments that enable capabilities, workloads, and governance to work and feel the same everywhere.</p>

<p>With the cohesion of Cloudera on cloud 7.2.18 and Cloudera on premises 7.1.9 into a streamlined 7.3.1 version, customers can now benefit from our accelerated pace of innovation with simpler versioning. This unified release provides a consistent feature set across all deployment environments â€“ whether on-premises or in the cloud.</p>

<p>This milestone release unleashes the power of true hybrid capabilities for data management, enabling data and analytics running on-premises and across multiple clouds to be managed cohesively and federated into a unified whole. Cloudera is the only platform transforming customersâ€™ data management and data journeys by enabling the seamless movement of data and workloads across all environments.</p>

<p><b><i>Welcome to the future of hybrid data solutions with Cloudera!</i></b></p>

<h1 id="release-highlights">Release Highlights:</h1>

<ul>
  <li>
    <p>Hive and Iceberg integration: The integration of Apache Hive with Apache Iceberg brings modern, open-table format capabilities to Hiveâ€™s powerful data warehousing features. With increased performance and scalability, Hive users can easily work with large, complex datasets across both cloud and on-premises environments. Icebergâ€™s support for schema evolution, time travel, and ACID compliance allows Hive users to manage data changes seamlessly and query historical data snapshots without compromising on performance. This powerful combination streamlines data management, enhances flexibility, and opens new doors for efficient data operations in hybrid environments.</p>
  </li>
  <li>
    <p>Graviton support (tech preview): This release supports AWS Graviton processors, allowing our platform to harness the powerful, cost-efficient capabilities of Arm-based EC2 instances. Customers can now achieve up to 40% better price performance for data-intensive workloads, making large-scale data processing both faster and more accessible. Graviton support not only enhances processing speeds and reduces operational costs, but it also aligns with our commitment to flexible, cloud-optimized performance.</p>
  </li>
  <li>
    <p>SQL Assistant:  Hue now leverages Large Language Models (LLMs) to transform data interaction with its SQL AI assistant. Generate SQL queries from natural language, optimize, explain, and fix them effortlessly. Compatible with OpenAIâ€™s GPT, Amazon Bedrock, and Azureâ€™s OpenAI, Hue offers the flexibility to choose the AI service that suits you best.</p>
  </li>
  <li>
    <p>Support for Amazon S3 Express One Zone Storage: Cloud connectors now offer high-performance, single-availability Zone storage with consistent single-digit millisecond access. By storing data in the same Availability Zone as your compute resources, you can optimize performance, reduce costs, and accelerate workloads. This storage class supports hundreds of thousands of requests per second for seamless scalability.</p>
  </li>
  <li>
    <p>Improved security posture: This release addresses 225+ total Common Vulnerabilities and Exposures (CVEs) which reflects our strong commitment to safeguarding data and ensuring a secure, resilient environment for our users. Each CVE resolution bolsters the integrity and stability of our platform, protecting against potential threats across all deployment environments â€“ whether on-premises or in the cloud, or hybrid.</p>
  </li>
</ul>

<h1 id="additional-features">Additional Features:</h1>
<ol>
  <li>Zero Downtime Upgrade (ZDU):  Building on the success of ZDU in version 7.1.9, by supporting ZDU in 7.3.1, we continue to simplify the upgrade process for hassle-free upgrades while maintaining high availability and minimizing disruptions to critical operations.</li>
  <li>Ozone HBase integration (tech preview): This integration provides an object storage solution for Apache HBase. On Ozone, HBase can now efficiently handle massive tables and provide random, realtime read/write access to your Big Data on S3 compliant storage.</li>
  <li>Cloudera Replication Manager Enhancements:
    <ul>
      <li>Advanced Iceberg replication to support Iceberg V2 tables created by Hive, Impala, and Spark</li>
      <li>Atlas Replication (tech preview) allows users to replicate Atlas metadata and data lineage for Hive external tables and Iceberg tables from on-premises to on-premises.</li>
      <li>RM extends ozone replication from data only to metadata; ozone metadata replication can be scheduled via Hive external table replication.</li>
    </ul>
  </li>
  <li>Spark 3 Standardization:  For customers moving from Spark 2, this release simplifies the transition through enhanced tooling to ease the migration process. By upgrading to Spark 3, users can unlock superior performance, better resource utilization, and the ability to seamlessly integrate with modern data and AI workloads across hybrid and multi-cloud environments.</li>
  <li>Unifying Cloudera On Cloud 7.2.18 and Cloudera On-Premises 7.1.9 now ensure feature consistency across deployment environments. For example, multi-authentication support for SAML and LDAP in the Apache Hive component is now available on Cloudera on-premises, while the OpenJPA 3 upgrade for Apache Oozie is now available on Cloudera On Cloud. For a complete list of new features on either platform, please refer to the release notes.</li>
</ol>

<h1 id="multi-python-support">Multi Python Support</h1>
<ul>
  <li>Python 3.9: RHEL/Oracle 8.x, RHEL/Oracle 9.x, and Ubuntu 20.</li>
  <li>Python 3.10: SLES 15 SP4 and SP5, and Ubuntu 22</li>
</ul>

<h1 id="upgrade-paths-to-cloudera-731">Upgrade Paths to Cloudera 7.3.1</h1>
<ul>
  <li>Cloudera on-premise: Direct upgrades (including downgrades and rollbacks) are supported from versions 7.1.9 SP1, 7.1.8, and 7.1.7 SP3.</li>
  <li>Cloudera on Cloud: Direct upgrades are supported from 7.2.18.0, 7.2.18.100, 7.2.18.300, and 7.2.17.200 through 7.2.17.500.</li>
  <li>Upgrades from other versions not listed above may require a two-hop upgrade.</li>
</ul>

<h1 id="operating-system-database-and-jdk-support">Operating System, Database and JDK Support</h1>
<p>RHEL 8.10: The latest release supports RHEL 8.10 for Cloudera on cloud.
JDK 17:  The latest release introduces JDK 17 support for Cloudera on cloud, delivering enhanced performance, security, and compatibility for modern cloud-native applications.
PostgreSQL 14: This support is now present for Cloudera on cloud.</p>

<h1 id="removed-components">Removed Components</h1>
<p>The following components have been removed and are no longer available in Cloudera 7.3.1:</p>
<ul>
  <li>Apache Spark 2 (see <a href="https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/runtime-release-notes/topics/rt-pvc-deprecated-spark2.html">Deprecation notice</a> for Apache Spark 2)</li>
  <li>Apache Livy 2 (see <a href="https://docs.cloudera.com/cdp-private-cloud-base/7.3.1/private-release-notes/topics/rt-deprecated-livy2.html">Deprecation notice</a> for Apache Livy 2)</li>
  <li>Apache Zeppelin (see <a href="https://docs.cloudera.com/cdp-private-cloud-base/7.3.1/private-release-notes/topics/rt-deprecated-livy2.html">Deprecation notice</a> for Apache Zeppelin)</li>
</ul>

<h1 id="cloudera-data-services-on-premises-support">Cloudera Data Services On Premises Support</h1>
<p>Cloudera Data Services on premises are currently not supported with 7.3.1 and support will be coming down the road in subsequent CHFs and service packs.  Customers using Cloudera Data Services on premises are suggested to stay with 7.1.9, as upgrading to 7.3.1 will break compatibility. These customers should upgrade to 7.3.1 at a future date when support is available.</p>

<p>Check out Cloudera Runtime <a href="https://docs.cloudera.com/cdp-private-cloud-base/7.3.1/private-release-notes/topics/rt-runtime-overview.html">Release Notes</a></p>

<p>Check out Public Cloud Runtime <a href="https://docs.cloudera.com/runtime/7.3.1/index.html">Release Notes</a></p>

<p>Check out <a href="https://docs.cloudera.com/cdp-private-cloud-base/7.3.1/private-release-notes/topics/fixed_common_vulnerabilities_exposures_731.html">Fixed CVEs in 7.3.1</a></p>]]></content><author><name>Steven Matison</name></author><category term="blog" /><category term="cloudera" /><category term="cdp" /><summary type="html"><![CDATA[We are thrilled to announce Clouderaâ€™s first-ever unified release: Cloudera 7.3.1! This is a single software release available for both cloud and on-premises environments that enable capabilities, workloads, and governance to work and feel the same everywhere.]]></summary></entry><entry><title type="html">Cloudera Streams Messaging Operator 1.1</title><link href="https://cldr-steven-matison.github.io//blog/Cloudera-Streams-Messaging-Operator-1.1/" rel="alternate" type="text/html" title="Cloudera Streams Messaging Operator 1.1" /><published>2024-09-06T00:00:00-04:00</published><updated>2024-09-06T00:00:00-04:00</updated><id>https://cldr-steven-matison.github.io//blog/Cloudera%20Streams%20Messaging%20Operator%201.1</id><content type="html" xml:base="https://cldr-steven-matison.github.io//blog/Cloudera-Streams-Messaging-Operator-1.1/"><![CDATA[<p>Moving rapidly here at Cloudera and the Streams Messaging Operator.  Happy to announce the latest release of Cloudera Streams Messaging Operator for Apache Kafka is now ready.   Lots of features including Kafka Replication without Mirrormaker 2.</p>

<p>See <a href="https://docs.cloudera.com/csm-operator/1.1/release-notes/topics/csm-op-rn.html">Whatâ€™s New</a> with Cloudera Streams Messaging Operator 1.1.</p>

<h1 id="release-of-cloudera-streams-messaging-operator-11">Release of Cloudera Streams Messaging Operator 1.1</h1>

<h2 id="key-features-for-this-release">Key Features For This Release</h2>

<ol>
  <li>Rebase to Strimzi 0.41.0</li>
  <li>Kafka Connect Support</li>
  <li>Kafka Replication Support</li>
</ol>

<p>For a complete list of features, fixes, issues, and unsupported features for this release see the <a href="https://docs.cloudera.com/csm-operator/1.1/release-notes/topics/csm-op-rn.html">Release Notes</a>.</p>

<p>Check out this <a href="https://community.cloudera.com/t5/What-s-New-Cloudera/Cloudera-Streams-Messaging-Operator-1-1/ba-p/393138">Cloudera Community Post</a> from @araujo.</p>

<p>Check out <a href="https://docs.cloudera.com/csm-operator/1.1/installation/topics/csm-op-install-overview.html">Installation Information</a>.</p>

<p>Check out <a href="https://docs.cloudera.com/csm-operator/1.1/upgrade/topics/csm-op-upgrade.html">Upgrade Information</a>.</p>

<p>As always, check out the entire <a href="https://docs.cloudera.com/csm-operator/1.1/index.html">Cloudera Streams Messaging Operator 1.1 Docs</a>.</p>]]></content><author><name>Steven Matison</name></author><category term="blog" /><category term="csm" /><category term="cloudera" /><category term="streams messaging" /><category term="operator" /><summary type="html"><![CDATA[Moving rapidly here at Cloudera and the Streams Messaging Operator. Happy to announce the latest release of Cloudera Streams Messaging Operator for Apache Kafka is now ready. Lots of features including Kafka Replication without Mirrormaker 2.]]></summary></entry><entry><title type="html">Cloudera Streaming Analytics 1.13 for Private Cloud Base</title><link href="https://cldr-steven-matison.github.io//blog/CSA-1.13-Release/" rel="alternate" type="text/html" title="Cloudera Streaming Analytics 1.13 for Private Cloud Base" /><published>2024-08-19T00:00:00-04:00</published><updated>2024-08-19T00:00:00-04:00</updated><id>https://cldr-steven-matison.github.io//blog/CSA-1.13-Release</id><content type="html" xml:base="https://cldr-steven-matison.github.io//blog/CSA-1.13-Release/"><![CDATA[<p>The Data In Motion team is pleased to announce the release of the Cloudera Streaming Analytics 1.13 for Cloudera Private Cloud Base 7.1.9 SP1. This release includes improvements to SQL Stream Builder as well as updates to Apache Flink 1.19.1. These changes are focused on enhancing the user experience and removing objections and blockers in the sales cycle.</p>

<p>See <a href="https://docs.cloudera.com/csa/latest/release-notes/topics/csa-what-new.html">Whatâ€™s New</a>.</p>

<h1 id="release-of-csa-113">Release of CSA 1.13</h1>

<h2 id="key-features">Key Features</h2>

<ol>
  <li>Rebase to Apache Flink 1.19.1</li>
  <li>Support for Python UDFs in SSB</li>
  <li>Global logging configuration for SSB jobs</li>
  <li>Customizable default Kafka TrustStore configuration in Streaming SQL Console</li>
  <li>Cloudera LTS Platform Support</li>
</ol>

<p>Check out <a href="https://docs.cloudera.com/csa/1.13.0/index.html">CSA 1.13</a></p>

<p>Check out <a href="https://docs.cloudera.com/csa/1.13.0/release-notes/topics/csa-what-new.html">Release Notes</a></p>

<p>Check out <a href="https://docs.cloudera.com/csa/1.13.0/download/topics/csa-download-location.html">Download Information</a></p>

<p>As always, check out the <a href="https://docs.cloudera.com/csa/1.13.0/index.html">CSA 1.13 Docs</a></p>]]></content><author><name>Steven Matison</name></author><category term="blog" /><category term="csa" /><category term="flink" /><category term="cdp" /><summary type="html"><![CDATA[The Data In Motion team is pleased to announce the release of the Cloudera Streaming Analytics 1.13 for Cloudera Private Cloud Base 7.1.9 SP1. This release includes improvements to SQL Stream Builder as well as updates to Apache Flink 1.19.1. These changes are focused on enhancing the user experience and removing objections and blockers in the sales cycle.]]></summary></entry><entry><title type="html">Installing Cloudera CFM Kubernetes Operator</title><link href="https://cldr-steven-matison.github.io//blog/Install-CFM-Operator/" rel="alternate" type="text/html" title="Installing Cloudera CFM Kubernetes Operator" /><published>2024-08-16T00:00:00-04:00</published><updated>2024-08-16T00:00:00-04:00</updated><id>https://cldr-steven-matison.github.io//blog/Install%20CFM%20Operator</id><content type="html" xml:base="https://cldr-steven-matison.github.io//blog/Install-CFM-Operator/"><![CDATA[<p>Last week I had a chance to work out the installation of the Clouderaâ€™s CFM Operator.  In this post I am going to expose the lessons learned and command required to get this CFM Operator running on my macbook with <a href="https://minikube.sigs.k8s.io/docs/start/">MiniKube</a>.  Keep in mind, these Operators are GA for RedHat Openshift. This demonstration on how to locally install is for evaluation purposes and not meant for actual usage.</p>

<figure>
  <img src="/assets/images/cfm-op-deployment-architecture.jpg" />
  <figcaption>CFM Deployment Architecture</figcaption>
</figure>

<p>First, lets start with the main page for the CFM Operator:</p>

<p><a href="https://docs.cloudera.com/cfm-operator/2.8.0/index.html">CFM Operator 2.8</a></p>

<p>Next lets take a look at some requirements:</p>

<ol>
  <li>Install Docker</li>
  <li>Install MiniKube</li>
  <li>Install Helm</li>
  <li>Install cfmctl</li>
</ol>

<p>Now, onto the installation pages for detailed steps and instructions:</p>

<ol>
  <li><a href="https://docs.cloudera.com/cfm-operator/2.8.0/installation/topics/cfm-op-install-overview.html">Installing CFM Operator</a></li>
  <li><a href="https://docs.cloudera.com/cfm-operator/2.8.0/nifi-deployment-configuration/topics/cfm-op-deploy-nifi-cluster.html">Installing Apache Nifi</a></li>
  <li><a href="https://docs.cloudera.com/cfm-operator/2.8.0/registry-deployment-configuration/topics/cfm-op-nifi-registry-deployment.html">Installing Apache NiFi Registry</a></li>
</ol>

<p>All the commands for this learning session are as follows:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">minikube</span> <span class="n">start</span>
<span class="n">helm</span> <span class="n">install</span> <span class="p">\\</span>
  <span class="n">cert</span><span class="o">-</span><span class="n">manager</span> <span class="n">jetstack</span><span class="o">/</span><span class="n">cert</span><span class="o">-</span><span class="n">manager</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">namespace</span> <span class="n">cert</span><span class="o">-</span><span class="n">manager</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">create</span><span class="o">-</span><span class="n">namespace</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">version</span> <span class="n">v1</span><span class="o">.</span><span class="mf">15.2</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">set</span> <span class="n">crds</span><span class="p">.</span><span class="nf">enabled</span><span class="o">=</span><span class="kp">true</span>
<span class="n">kubectl</span> <span class="n">create</span> <span class="n">namespace</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="nb">system</span>
<span class="n">kubectl</span> <span class="n">create</span> <span class="n">secret</span> <span class="n">docker</span><span class="o">-</span><span class="n">registry</span> <span class="n">docker</span><span class="o">-</span><span class="n">pull</span><span class="o">-</span><span class="n">secret</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">namespace</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="nb">system</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">docker</span><span class="o">-</span><span class="n">server</span> <span class="n">container</span><span class="p">.</span><span class="nf">repository</span><span class="p">.</span><span class="nf">cloudera</span><span class="p">.</span><span class="nf">com</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">docker</span><span class="o">-</span><span class="n">username</span> <span class="p">[</span><span class="no">License</span> <span class="no">Username</span><span class="p">]</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">docker</span><span class="o">-</span><span class="n">password</span> <span class="p">[</span><span class="no">License</span> <span class="no">Password</span><span class="p">]</span>
<span class="p">.</span><span class="nf">/</span><span class="n">cfmctl</span> <span class="n">install</span> <span class="o">--</span><span class="n">license</span> <span class="p">.</span><span class="nf">/</span><span class="n">license</span><span class="p">.</span><span class="nf">txt</span> <span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="n">repository</span> <span class="s2">"container.repository.cloudera.com/cloudera/cfm-operator"</span> <span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="n">tag</span> <span class="s2">"2.8.0-b94"</span> <span class="o">--</span><span class="n">namespace</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="nb">system</span>
<span class="n">kubectl</span> <span class="n">create</span> <span class="n">namespace</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="n">cluster</span>
<span class="n">kubectl</span> <span class="n">create</span> <span class="n">secret</span> <span class="n">docker</span><span class="o">-</span><span class="n">registry</span> <span class="n">docker</span><span class="o">-</span><span class="n">pull</span><span class="o">-</span><span class="n">secret</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">namespace</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="n">cluster</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">docker</span><span class="o">-</span><span class="n">server</span> <span class="n">container</span><span class="p">.</span><span class="nf">repository</span><span class="p">.</span><span class="nf">cloudera</span><span class="p">.</span><span class="nf">com</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">docker</span><span class="o">-</span><span class="n">username</span> <span class="p">[</span><span class="no">License</span> <span class="no">Username</span><span class="p">]</span> <span class="p">\\</span>
  <span class="o">--</span><span class="n">docker</span><span class="o">-</span><span class="n">password</span> <span class="p">[</span><span class="no">License</span> <span class="no">Password</span><span class="p">]</span>
<span class="n">kubectl</span> <span class="n">apply</span> <span class="o">-</span><span class="n">f</span> <span class="n">nifi</span><span class="p">.</span><span class="nf">yaml</span> <span class="o">--</span><span class="n">namespace</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="n">cluster</span>
</code></pre></div></div>

<p>The source of my nifi.yaml:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ss">apiVersion: </span><span class="n">cfm</span><span class="p">.</span><span class="nf">cloudera</span><span class="p">.</span><span class="nf">com</span><span class="o">/</span><span class="n">v1alpha1</span>
<span class="ss">kind: </span><span class="no">Nifi</span>
<span class="ss">metadata:
  name: </span><span class="n">mynifi</span>
<span class="ss">spec:
  replicas: </span><span class="mi">1</span>
  <span class="ss">image:
    repository: </span><span class="n">container</span><span class="p">.</span><span class="nf">repository</span><span class="p">.</span><span class="nf">cloudera</span><span class="p">.</span><span class="nf">com</span><span class="o">/</span><span class="n">cloudera</span><span class="o">/</span><span class="n">cfm</span><span class="o">-</span><span class="n">nifi</span><span class="o">-</span><span class="n">k8s</span>
    <span class="ss">tag: </span><span class="mf">2.8</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="n">b94</span><span class="o">-</span><span class="n">nifi_1</span><span class="o">.</span><span class="mf">25.0</span><span class="o">.</span><span class="mf">2.3</span><span class="o">.</span><span class="mf">13.0</span><span class="o">-</span><span class="mi">36</span>
    <span class="ss">pullSecret: </span><span class="n">docker</span><span class="o">-</span><span class="n">pull</span><span class="o">-</span><span class="n">secret</span>
  <span class="ss">tiniImage:
    repository: </span><span class="n">container</span><span class="p">.</span><span class="nf">repository</span><span class="p">.</span><span class="nf">cloudera</span><span class="p">.</span><span class="nf">com</span><span class="o">/</span><span class="n">cloudera</span><span class="o">/</span><span class="n">cfm</span><span class="o">-</span><span class="n">tini</span>
    <span class="ss">tag: </span><span class="mf">2.8</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="n">b94</span>
    <span class="ss">pullSecret: </span><span class="n">docker</span><span class="o">-</span><span class="n">pull</span><span class="o">-</span><span class="n">secret</span>
  <span class="ss">hostName: </span><span class="n">mynifi</span><span class="p">.</span><span class="nf">localhost</span>
  <span class="ss">uiConnection:
    type: </span><span class="no">Ingress</span>
  <span class="ss">configOverride:
    nifiProperties:
      upsert:
        </span><span class="n">nifi</span><span class="p">.</span><span class="nf">cluster</span><span class="p">.</span><span class="nf">leader</span><span class="p">.</span><span class="nf">election</span><span class="p">.</span><span class="nf">implementation</span><span class="p">:</span> <span class="s2">"KubernetesLeaderElectionManager"</span>
    <span class="ss">authorizers: </span><span class="o">|</span>
      <span class="o">&lt;</span><span class="n">authorizers</span><span class="o">&gt;</span>
        <span class="o">&lt;</span><span class="n">authorizer</span><span class="o">&gt;</span>
          <span class="o">&lt;</span><span class="n">identifier</span><span class="o">&gt;</span><span class="n">single</span><span class="o">-</span><span class="n">user</span><span class="o">-</span><span class="n">authorizer</span><span class="o">&lt;</span><span class="sr">/identifier&gt;
          &lt;class&gt;org.apache.nifi.authorization.single.user.SingleUserAuthorizer&lt;/</span><span class="k">class</span><span class="o">&gt;</span>
        <span class="o">&lt;</span><span class="sr">/authorizer&gt;
      &lt;/</span><span class="n">authorizers</span><span class="o">&gt;</span>
    <span class="ss">loginIdentityProviders: </span><span class="o">|</span>
      <span class="o">&lt;</span><span class="n">loginIdentityProviders</span><span class="o">&gt;</span>
        <span class="o">&lt;</span><span class="n">provider</span><span class="o">&gt;</span>
          <span class="o">&lt;</span><span class="n">identifier</span><span class="o">&gt;</span><span class="n">single</span><span class="o">-</span><span class="n">user</span><span class="o">-</span><span class="n">provider</span><span class="o">&lt;</span><span class="sr">/identifier&gt;
          &lt;class&gt;org.apache.nifi.authentication.single.user.SingleUserLoginIdentityProvider&lt;/</span><span class="k">class</span><span class="o">&gt;</span>
          <span class="o">&lt;</span><span class="n">property</span> <span class="nb">name</span><span class="o">=</span><span class="s2">"Username"</span><span class="o">&gt;</span><span class="n">admin</span><span class="o">&lt;</span><span class="sr">/property&gt;
          &lt;property name="Password"&gt;$2b$10$GRa8g9Z5rBENXPFNHFBosev9XmY6CSk0SdcBi5sQMRX92KD73asGG&lt;/</span><span class="n">property</span><span class="o">&gt;</span>
        <span class="o">&lt;</span><span class="sr">/provider&gt;
      &lt;/</span><span class="n">loginIdentityProviders</span><span class="o">&gt;</span>
  <span class="ss">stateManagement:
   clusterProvider:
     id: </span><span class="n">kubernetes</span><span class="o">-</span><span class="n">provider</span>
     <span class="ss">class: </span><span class="n">org</span><span class="p">.</span><span class="nf">apache</span><span class="p">.</span><span class="nf">nifi</span><span class="p">.</span><span class="nf">kubernetes</span><span class="p">.</span><span class="nf">state</span><span class="p">.</span><span class="nf">provider</span><span class="o">.</span><span class="no">KubernetesConfigMapStateProvider</span>

</code></pre></div></div>

<p>Check out some commands I needed to see what was happening during install:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kubectl</span> <span class="n">get</span> <span class="n">pods</span> <span class="o">--</span><span class="n">all</span><span class="o">-</span><span class="n">namespaces</span>
<span class="n">kubectl</span> <span class="n">get</span> <span class="n">pods</span> <span class="o">--</span><span class="n">all</span><span class="o">-</span><span class="n">namespaces</span> <span class="o">-</span><span class="n">o</span> <span class="n">wide</span>
<span class="n">kubectl</span> <span class="n">describe</span> <span class="n">pod</span> <span class="n">mynifi</span><span class="o">-</span><span class="mi">0</span> <span class="o">-</span><span class="n">n</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="n">cluster</span>
<span class="n">kubectl</span> <span class="n">logs</span> <span class="o">-</span><span class="n">n</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="nb">system</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="mi">664598</span><span class="n">bf69</span><span class="o">-</span><span class="n">rm2lx</span>
<span class="n">kubectl</span> <span class="n">get</span> <span class="n">events</span> <span class="o">-</span><span class="n">n</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span>
<span class="n">kubectl</span> <span class="n">get</span> <span class="n">events</span> <span class="o">-</span><span class="n">n</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="nb">system</span>
<span class="n">kubectl</span> <span class="n">get</span> <span class="n">events</span> <span class="o">-</span><span class="n">n</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="n">cluster</span>
<span class="n">kubectl</span> <span class="n">get</span> <span class="n">secrets</span> <span class="o">-</span><span class="n">n</span> <span class="n">cfm</span><span class="o">-</span><span class="n">operator</span><span class="o">-</span><span class="nb">system</span>
</code></pre></div></div>

<p>Things to watch out for:</p>

<ol>
  <li>When copy/pasting commands make sure you fix return lines and \ syntax.</li>
  <li>Be sure to get your correct License Username and Password with proper entitlements for these operators.</li>
  <li>Allocate enough resources for your kubernetes cluster in docker. Recommend 16gb or more memory.</li>
  <li>Ensure you have created the docker secret in both the operator and cluster namespaces.</li>
</ol>

<p>And last but not least, my full kubernetes cluster after install:</p>

<figure>
  <img src="/assets/images/cfm-kubectl-get-pods.png" />
  <figcaption>CFM Operator and NiFi Deployment</figcaption>
</figure>

<p>If you are interested in getting your hands on the NiFi Operators you can find more about Cloudera DataFlow starting right <a href="https://www.cloudera.com/products/dataflow.html">here</a>.</p>]]></content><author><name>Steven Matison</name></author><category term="blog" /><category term="kubernetes" /><category term="operator" /><category term="cfm" /><summary type="html"><![CDATA[Last week I had a chance to work out the installation of the Clouderaâ€™s CFM Operator. In this post I am going to expose the lessons learned and command required to get this CFM Operator running on my macbook with MiniKube. Keep in mind, these Operators are GA for RedHat Openshift. This demonstration on how to locally install is for evaluation purposes and not meant for actual usage.]]></summary></entry></feed>